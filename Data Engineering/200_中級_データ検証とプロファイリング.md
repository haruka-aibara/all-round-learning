# データ検証とプロファイリング (難易度レベル: 200)

## 概要
データ検証とプロファイリングは、データの品質を確保し、データの特性を理解するための重要なプロセスです。データ駆動型の意思決定や機械学習モデルの構築において、信頼性の高いデータは不可欠です。この技術により、データの整合性、完全性、正確性を確認し、データの特性を把握することで、より効果的なデータ活用が可能になります。

## 詳細

### データ検証の重要性
データ検証は、データが期待される形式、範囲、品質基準を満たしているかを確認するプロセスです。適切なデータ検証により、以下の問題を防ぐことができます：

- **データの不整合**: 異なるソースからのデータの形式や単位の違い
- **欠損値や異常値**: データの不完全性や不正な値の混入
- **ビジネスルール違反**: ドメイン固有の制約条件の違反
- **データ型の不一致**: 期待されるデータ型と実際のデータ型の相違

### データプロファイリングの目的
データプロファイリングは、データセットの構造、内容、品質を分析するプロセスです。以下の情報を提供します：

- **データの統計的概要**: 基本統計量、分布、相関関係
- **データ品質の評価**: 欠損値、重複、異常値の検出
- **データの構造理解**: スキーマ、データ型、制約条件
- **データの特性把握**: パターン、トレンド、外れ値

### データ検証の手法

#### 1. スキーマ検証
データの構造と形式を確認する基本的な検証手法

```python
import pandas as pd
from typing import Dict, Any

class SchemaValidator:
    def __init__(self, expected_schema: Dict[str, Any]):
        self.expected_schema = expected_schema
    
    def validate_schema(self, df: pd.DataFrame) -> Dict[str, bool]:
        """データフレームのスキーマを検証"""
        validation_results = {}
        
        for column, expected_type in self.expected_schema.items():
            if column not in df.columns:
                validation_results[column] = False
                continue
            
            actual_type = str(df[column].dtype)
            validation_results[column] = actual_type == expected_type
        
        return validation_results

# 使用例
expected_schema = {
    'user_id': 'int64',
    'name': 'object',
    'email': 'object',
    'age': 'int64',
    'created_at': 'datetime64[ns]'
}

validator = SchemaValidator(expected_schema)
results = validator.validate_schema(user_data)
```

#### 2. 範囲検証
データ値が期待される範囲内にあるかを確認

```python
class RangeValidator:
    def __init__(self, column_ranges: Dict[str, tuple]):
        self.column_ranges = column_ranges
    
    def validate_ranges(self, df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:
        """各カラムの値が指定された範囲内にあるかを検証"""
        results = {}
        
        for column, (min_val, max_val) in self.column_ranges.items():
            if column not in df.columns:
                continue
            
            # 数値型の場合
            if pd.api.types.is_numeric_dtype(df[column]):
                out_of_range = df[(df[column] < min_val) | (df[column] > max_val)]
                results[column] = {
                    'valid': len(out_of_range) == 0,
                    'out_of_range_count': len(out_of_range),
                    'min_found': df[column].min(),
                    'max_found': df[column].max()
                }
            
            # 文字列型の場合（長さチェック）
            elif pd.api.types.is_string_dtype(df[column]):
                string_lengths = df[column].str.len()
                out_of_range = df[(string_lengths < min_val) | (string_lengths > max_val)]
                results[column] = {
                    'valid': len(out_of_range) == 0,
                    'out_of_range_count': len(out_of_range),
                    'min_length': string_lengths.min(),
                    'max_length': string_lengths.max()
                }
        
        return results

# 使用例
column_ranges = {
    'age': (0, 120),
    'email': (5, 100),  # 文字列長
    'score': (0, 100)
}

range_validator = RangeValidator(column_ranges)
range_results = range_validator.validate_ranges(user_data)
```

#### 3. 形式検証
データが特定の形式（正規表現、パターン）に従っているかを確認

```python
import re

class FormatValidator:
    def __init__(self, format_patterns: Dict[str, str]):
        self.format_patterns = format_patterns
    
    def validate_formats(self, df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:
        """各カラムの値が指定された形式に従っているかを検証"""
        results = {}
        
        for column, pattern in self.format_patterns.items():
            if column not in df.columns:
                continue
            
            # 正規表現パターンでマッチング
            matches = df[column].astype(str).str.match(pattern)
            invalid_count = (~matches).sum()
            
            results[column] = {
                'valid': invalid_count == 0,
                'invalid_count': invalid_count,
                'valid_percentage': (matches.sum() / len(df)) * 100
            }
        
        return results

# 使用例
format_patterns = {
    'email': r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$',
    'phone': r'^\d{3}-\d{3}-\d{4}$',
    'postal_code': r'^\d{5}(-\d{4})?$'
}

format_validator = FormatValidator(format_patterns)
format_results = format_validator.validate_formats(user_data)
```

### データプロファイリングの手法

#### 1. 基本統計量の計算
データの分布と特性を理解するための統計情報

```python
class DataProfiler:
    def __init__(self):
        pass
    
    def calculate_basic_stats(self, df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:
        """各カラムの基本統計量を計算"""
        stats = {}
        
        for column in df.columns:
            col_stats = {}
            
            # データ型に応じた統計量
            if pd.api.types.is_numeric_dtype(df[column]):
                col_stats.update({
                    'count': df[column].count(),
                    'mean': df[column].mean(),
                    'std': df[column].std(),
                    'min': df[column].min(),
                    'max': df[column].max(),
                    'median': df[column].median(),
                    'missing_count': df[column].isnull().sum(),
                    'missing_percentage': (df[column].isnull().sum() / len(df)) * 100
                })
            
            elif pd.api.types.is_string_dtype(df[column]):
                col_stats.update({
                    'count': df[column].count(),
                    'unique_count': df[column].nunique(),
                    'most_common': df[column].mode().iloc[0] if not df[column].mode().empty else None,
                    'most_common_count': df[column].value_counts().iloc[0] if not df[column].value_counts().empty else 0,
                    'missing_count': df[column].isnull().sum(),
                    'missing_percentage': (df[column].isnull().sum() / len(df)) * 100,
                    'min_length': df[column].str.len().min(),
                    'max_length': df[column].str.len().max()
                })
            
            stats[column] = col_stats
        
        return stats
```

#### 2. データ品質の評価
データの品質指標を計算

```python
class DataQualityAssessor:
    def __init__(self):
        pass
    
    def assess_quality(self, df: pd.DataFrame) -> Dict[str, float]:
        """データ品質の総合評価"""
        quality_metrics = {}
        
        # 完全性（欠損値の少なさ）
        completeness = 1 - (df.isnull().sum().sum() / (len(df) * len(df.columns)))
        quality_metrics['completeness'] = completeness
        
        # 一意性（重複の少なさ）
        uniqueness = 1 - (len(df) - len(df.drop_duplicates())) / len(df)
        quality_metrics['uniqueness'] = uniqueness
        
        # 整合性（データ型の一貫性）
        consistency_score = 0
        for column in df.columns:
            if pd.api.types.is_numeric_dtype(df[column]):
                # 数値型の場合、異常値の割合をチェック
                Q1 = df[column].quantile(0.25)
                Q3 = df[column].quantile(0.75)
                IQR = Q3 - Q1
                outliers = df[(df[column] < Q1 - 1.5 * IQR) | (df[column] > Q3 + 1.5 * IQR)]
                consistency_score += 1 - (len(outliers) / len(df))
        
        consistency_score /= len(df.columns)
        quality_metrics['consistency'] = consistency_score
        
        # 総合品質スコア
        quality_metrics['overall_quality'] = (
            quality_metrics['completeness'] * 0.4 +
            quality_metrics['uniqueness'] * 0.3 +
            quality_metrics['consistency'] * 0.3
        )
        
        return quality_metrics
```

#### 3. 相関分析
カラム間の関係性を分析

```python
class CorrelationAnalyzer:
    def __init__(self):
        pass
    
    def analyze_correlations(self, df: pd.DataFrame, threshold: float = 0.5) -> Dict[str, Any]:
        """数値カラム間の相関関係を分析"""
        # 数値カラムのみを選択
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        
        if len(numeric_columns) < 2:
            return {'message': '相関分析に必要な数値カラムが不足しています'}
        
        # 相関行列を計算
        correlation_matrix = df[numeric_columns].corr()
        
        # 強い相関関係を抽出
        strong_correlations = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i + 1, len(correlation_matrix.columns)):
                corr_value = correlation_matrix.iloc[i, j]
                if abs(corr_value) >= threshold:
                    strong_correlations.append({
                        'column1': correlation_matrix.columns[i],
                        'column2': correlation_matrix.columns[j],
                        'correlation': corr_value
                    })
        
        return {
            'correlation_matrix': correlation_matrix,
            'strong_correlations': strong_correlations,
            'threshold': threshold
        }
```

## 具体例

### 実装例：統合データ検証システム
```python
class ComprehensiveDataValidator:
    def __init__(self, schema: Dict[str, Any], ranges: Dict[str, tuple], 
                 formats: Dict[str, str]):
        self.schema_validator = SchemaValidator(schema)
        self.range_validator = RangeValidator(ranges)
        self.format_validator = FormatValidator(formats)
    
    def validate_all(self, df: pd.DataFrame) -> Dict[str, Any]:
        """包括的なデータ検証を実行"""
        results = {
            'schema_validation': self.schema_validator.validate_schema(df),
            'range_validation': self.range_validator.validate_ranges(df),
            'format_validation': self.format_validator.validate_formats(df)
        }
        
        # 総合評価
        all_valid = all(
            all(result for result in validation.values())
            for validation in results.values()
        )
        
        results['overall_valid'] = all_valid
        return results

# 使用例
schema = {
    'user_id': 'int64',
    'name': 'object',
    'email': 'object',
    'age': 'int64'
}

ranges = {
    'age': (0, 120),
    'name': (1, 50)
}

formats = {
    'email': r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
}

validator = ComprehensiveDataValidator(schema, ranges, formats)
validation_results = validator.validate_all(user_data)
```

### 実装例：データプロファイリングレポート生成
```python
class DataProfilingReport:
    def __init__(self):
        self.profiler = DataProfiler()
        self.quality_assessor = DataQualityAssessor()
        self.correlation_analyzer = CorrelationAnalyzer()
    
    def generate_report(self, df: pd.DataFrame) -> Dict[str, Any]:
        """包括的なデータプロファイリングレポートを生成"""
        report = {
            'dataset_info': {
                'rows': len(df),
                'columns': len(df.columns),
                'memory_usage': df.memory_usage(deep=True).sum()
            },
            'basic_stats': self.profiler.calculate_basic_stats(df),
            'quality_metrics': self.quality_assessor.assess_quality(df),
            'correlations': self.correlation_analyzer.analyze_correlations(df)
        }
        
        return report
    
    def print_summary(self, report: Dict[str, Any]):
        """レポートの要約を出力"""
        print("=== データセット概要 ===")
        print(f"行数: {report['dataset_info']['rows']}")
        print(f"列数: {report['dataset_info']['columns']}")
        print(f"メモリ使用量: {report['dataset_info']['memory_usage'] / 1024:.2f} KB")
        
        print("\n=== データ品質 ===")
        quality = report['quality_metrics']
        print(f"完全性: {quality['completeness']:.2%}")
        print(f"一意性: {quality['uniqueness']:.2%}")
        print(f"整合性: {quality['consistency']:.2%}")
        print(f"総合品質: {quality['overall_quality']:.2%}")
        
        print("\n=== 強い相関関係 ===")
        for corr in report['correlations']['strong_correlations']:
            print(f"{corr['column1']} - {corr['column2']}: {corr['correlation']:.3f}")

# 使用例
report_generator = DataProfilingReport()
report = report_generator.generate_report(sales_data)
report_generator.print_summary(report)
```

### 実践的な使用例

#### 1. ETL処理でのデータ検証
```python
def validate_etl_data(raw_data: pd.DataFrame, target_schema: Dict[str, Any]) -> bool:
    """ETL処理でのデータ検証"""
    validator = ComprehensiveDataValidator(
        schema=target_schema,
        ranges={'amount': (0, 1000000), 'quantity': (0, 10000)},
        formats={'order_id': r'^ORD-\d{6}$'}
    )
    
    results = validator.validate_all(raw_data)
    
    if not results['overall_valid']:
        # 検証エラーの詳細をログに記録
        log_validation_errors(results)
        return False
    
    return True
```

#### 2. 機械学習前処理でのデータプロファイリング
```python
def profile_ml_dataset(df: pd.DataFrame) -> Dict[str, Any]:
    """機械学習用データセットのプロファイリング"""
    profiler = DataProfiler()
    quality_assessor = DataQualityAssessor()
    
    # 基本統計量
    stats = profiler.calculate_basic_stats(df)
    
    # データ品質評価
    quality = quality_assessor.assess_quality(df)
    
    # 欠損値の詳細分析
    missing_analysis = {}
    for column in df.columns:
        missing_count = df[column].isnull().sum()
        if missing_count > 0:
            missing_analysis[column] = {
                'count': missing_count,
                'percentage': (missing_count / len(df)) * 100
            }
    
    return {
        'statistics': stats,
        'quality': quality,
        'missing_values': missing_analysis,
        'recommendations': generate_recommendations(stats, quality, missing_analysis)
    }
```

## まとめ

### 重要なポイント
- **データ検証**: スキーマ、範囲、形式の多角的な検証が重要
- **データプロファイリング**: 統計的概要と品質評価の両方が必要
- **継続的モニタリング**: データ品質の定期的な監視と改善
- **自動化**: 検証とプロファイリングの自動化による効率化

### 学んだことの振り返り
データ検証とプロファイリングは、信頼性の高いデータ活用の基盤となる技術です。適切な検証とプロファイリングにより、データの品質を確保し、より効果的な分析や機械学習モデルの構築が可能になります。

### 次のステップへの提案
1. **実装演習**: 実際のデータセットを使用した検証・プロファイリングシステムの構築
2. **品質指標の拡張**: ドメイン固有の品質指標の追加
3. **可視化**: プロファイリング結果の視覚的表現の実装
4. **自動化**: CI/CDパイプラインでのデータ品質チェックの統合 
